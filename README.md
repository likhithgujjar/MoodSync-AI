
# ğŸ§ Moodsync â€“ Emotion-Based Music Recommender ğŸ¶

Moodsync is an intelligent, real-time **emotion detection and music recommendation system** that analyzes a user's mood through their webcam and voice, and syncs music playback from YouTube Music accordingly.

> ğŸ˜„ ğŸ˜ ğŸ˜¢ ğŸ˜  â€” Your mood drives your music.

---

## ğŸš€ Features

- ğŸ§  **Dual Emotion Detection**: Combines predictions from a fine-tuned VGG16 CNN model and DeepFace.
- ğŸ“· **Real-time Webcam Analysis**: Detects facial expressions live using OpenCV and DeepFace.
- ğŸ™ï¸ **Voice Assistant â€“ Nova**: Triggered by "Hey Nova", supports commands like "Play song", "Pause", "Next", etc.
- ğŸµ **Music Integration**: Automatically skips disliked songs and selects songs from an emotion-tagged playlist (from `songss.xlsx`).
- ğŸ’» **GUI App**: Built with PyQt5 for a modern desktop experience with camera feed and logs.
- ğŸ¬ **Dataset Collection Tool**: Captures and labels face images based on emotion-induced audio (for training).

---

## ğŸ› ï¸ Tech Stack

| Component         | Technology                                  |
|------------------|---------------------------------------------|
| UI / App         | Python, PyQt5, OpenCV, DeepFace             |
| ML Model         | TensorFlow (VGG16 Fine-Tuned Model)         |
| Voice Assistant  | SpeechRecognition, pyttsx3                  |
| Music Control    | Selenium (YouTube Music Web Interface)      |
| Dataset Creation | Pygame, OpenCV                              |
| Dataset Format   | `emotion_dataset/`, `emotion_dataset_split/`|

---

## ğŸ“‚ Project Structure

```
ğŸ“ Moodsync/
â”œâ”€â”€ App.py                     # Main GUI application
â”œâ”€â”€ Model-FineTuner.py         # Train and fine-tune VGG16 model
â”œâ”€â”€ ModelTester.py             # Live testing of the trained model
â”œâ”€â”€ UserDataSetModeller.py     # Script to generate training dataset via webcam & audio
â”œâ”€â”€ 3-emotion_recognition_model.h5  # Trained emotion model (not included here)
â”œâ”€â”€ songss.xlsx                # Emotion-based song links
â”œâ”€â”€ *.mp3                      # Emotion-inducing audio files
```

---

## ğŸ”§ Setup Instructions

1. **Clone the Repository**
```bash
git clone https://github.com/your-username/moodsync.git
cd moodsync
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

Recommended libraries:
- opencv-python
- PyQt5
- selenium
- tensorflow
- deepface
- pygame
- qdarkstyle
- speechrecognition
- pyttsx3
- webdriver-manager
- openpyxl

3. **Prepare WebDriver**
> Chrome is used via `webdriver-manager`. No manual driver setup is needed.

---

## â–¶ï¸ How to Run

### Run the GUI App:
```bash
python App.py
```

### Generate Dataset (with audio prompts):
```bash
python UserDataSetModeller.py
```

### Train Custom Emotion Model:
```bash
python Model-FineTuner.py
```

### Test Emotion Detection Model:
```bash
python ModelTester.py
```

---

## ğŸ“Š Emotion Classes

- `happy`
- `sad`
- `neutral`
- `disgusted`

These emotions correspond to the dataset folders and song categories in `songss.xlsx`.

---

## ğŸ“Œ Notes

- Emotion detection combines both DeepFace and a custom-trained VGG16 model.
- The song recommendation system is rule-based using a pre-defined Excel sheet (`songss.xlsx`).
- Voice control requires a working microphone and quiet background.

---

## ğŸ™Œ Credits

Developed by Likhith Gujjar A  , Manasa SN ,Prince Jain.
Project inspired by emotion-aware computing and music therapy.

---

## ğŸªª License

MIT License â€“ free to use and modify.
